Test Accuracy: 93.24%
Confusion Matrix:
[[ 669    1    3   44]
 [   0   52    0    0]
 [   9    5 2305  241]
 [  12    4   27 1749]]
Classification Report:
              precision    recall  f1-score   support

           0       0.97      0.93      0.95       717
           1       0.84      1.00      0.91        52
           2       0.99      0.90      0.94      2560
           3       0.86      0.98      0.91      1792

    accuracy                           0.93      5121
   macro avg       0.91      0.95      0.93      5121
weighted avg       0.94      0.93      0.93      5121


import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Set paths for dataset
train_dir = r'C:\Users\soham\OneDrive\Desktop\hackathon\Dataset'
test_dir = r'C:\Users\soham\OneDrive\Desktop\hackathon\archive (1)\Alzheimer_s Dataset\train'

# Image parameters
img_size = (224, 224)
batch_size = 32

# Rescaling for training and testing (no augmentation)
data_gen_train = ImageDataGenerator(rescale=1.0 / 255)
data_gen_test = ImageDataGenerator(rescale=1.0 / 255)

# Load training data
train_data = data_gen_train.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

# Load testing data
test_data = data_gen_test.flow_from_directory(
    test_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

# Load DenseNet201 model for feature extraction
densenet_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
densenet_model.trainable = False

# Feature extraction function
def extract_features(data_generator, model):
    features = []
    labels = []
    total_batches = len(data_generator)
    print("Extracting features...")
    for i in range(total_batches):
        x_batch, y_batch = next(data_generator)
        feature_batch = model.predict(x_batch, verbose=0)
        features.append(feature_batch)
        labels.append(y_batch)
        print(f"Progress: {i + 1}/{total_batches}", end="\r")
    features = np.concatenate(features)
    labels = np.concatenate(labels)
    return features, labels

# Extract features and labels for training and testing
train_features, train_labels = extract_features(train_data, densenet_model)
test_features, test_labels = extract_features(test_data, densenet_model)

# Flatten features for ANN
train_features_flat = train_features.reshape(train_features.shape[0], -1)
test_features_flat = test_features.reshape(test_features.shape[0], -1)

# Reduce dimensionality using PCA
print("Applying PCA...")
pca = PCA(n_components=2000)
train_features_flat = pca.fit_transform(train_features_flat)
test_features_flat = pca.transform(test_features_flat)

# Convert labels from one-hot to single integer
train_labels_int = np.argmax(train_labels, axis=1)
test_labels_int = np.argmax(test_labels, axis=1)

# Define ANN model
ann_model = Sequential([
    Dense(512, input_dim=train_features_flat.shape[1], activation='relu'),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(train_labels.shape[1], activation='softmax')  # Output layer (softmax for multi-class classification)
])

# Compile the model
ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
print("Training the ANN model...")
history = ann_model.fit(
    train_features_flat, 
    train_labels, 
    epochs=200,  # Set epochs to 100
    batch_size=batch_size, 
    validation_data=(test_features_flat, test_labels)
)

# Evaluate the model
accuracy = ann_model.evaluate(test_features_flat, test_labels, verbose=0)
print(f"Test Accuracy: {accuracy[1] * 100:.2f}%")

# Make predictions on test set
predictions = np.argmax(ann_model.predict(test_features_flat), axis=1)

# Confusion matrix
conf_matrix = confusion_matrix(test_labels_int, predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=train_data.class_indices.keys(), yticklabels=train_data.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Classification report
print("Classification Report:")
print(classification_report(test_labels_int, predictions, zero_division=1))

# Plot Training Loss vs Test Loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Train Loss vs Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Training Accuracy vs Test Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Train Accuracy vs Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()