Test Accuracy: 80.69%
Confusion Matrix:
[[120   0  21  38]
 [  0  56   0   8]
 [ 10   2 551  77]
 [ 12   0  89 347]]
Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.67      0.75       179
           1       0.97      0.88      0.92        64
           2       0.83      0.86      0.85       640
           3       0.74      0.77      0.76       448

    accuracy                           0.81      1331
   macro avg       0.85      0.80      0.82      1331
weighted avg       0.81      0.81      0.81      1331


code :
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.utils import class_weight

# Set paths for dataset
train_dir = r'C:\Users\soham\OneDrive\Desktop\hackathon\dataset1_non_aug\Dataset'
test_dir = r'C:\Users\soham\OneDrive\Desktop\hackathon\archive (1)\Alzheimer_s Dataset\test'

# Image parameters
img_size = (224, 224)
batch_size = 32

# Data augmentation for training
data_gen_train = ImageDataGenerator(
    rescale=1.0/255,
    horizontal_flip=True,
    zoom_range=0.2,
    rotation_range=20,
    shear_range=0.2
)

# Rescaling for testing
data_gen_test = ImageDataGenerator(rescale=1.0/255)

# Load training data
train_data = data_gen_train.flow_from_directory(
    train_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

# Load testing data
test_data = data_gen_test.flow_from_directory(
    test_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

# Load DenseNet201 model for feature extraction
densenet_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
densenet_model.trainable = False

# Feature extraction function
def extract_features(data_generator, model):
    features = []
    labels = []
    total_batches = len(data_generator)
    print("Extracting features...")
    for i in range(total_batches):
        x_batch, y_batch = next(data_generator)
        feature_batch = model.predict(x_batch, verbose=0)
        features.append(feature_batch)
        labels.append(y_batch)
        print(f"Progress: {i + 1}/{total_batches}", end="\r")
    features = np.concatenate(features)
    labels = np.concatenate(labels)
    return features, labels

# Extract features and labels for training and testing
train_features, train_labels = extract_features(train_data, densenet_model)
test_features, test_labels = extract_features(test_data, densenet_model)

# Flatten features for ANN
train_features_flat = train_features.reshape(train_features.shape[0], -1)
test_features_flat = test_features.reshape(test_features.shape[0], -1)

# Reduce dimensionality using PCA
print("Applying PCA...")
pca = PCA(n_components=500)
train_features_flat = pca.fit_transform(train_features_flat)
test_features_flat = pca.transform(test_features_flat)

# Convert labels from one-hot to single integer
train_labels_int = np.argmax(train_labels, axis=1)
test_labels_int = np.argmax(test_labels, axis=1)

# Compute class weights to handle class imbalance
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels_int),
    y=train_labels_int
)
class_weights_dict = dict(enumerate(class_weights))

# Define ANN model
ann_model = Sequential([
    Dense(512, input_dim=train_features_flat.shape[1], activation='relu'),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(train_labels.shape[1], activation='softmax')  # Output layer (softmax for multi-class classification)
])

# Compile the model
ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
print("Training the ANN model...")
history = ann_model.fit(train_features_flat, train_labels, epochs=500, batch_size=batch_size, 
                        validation_data=(test_features_flat, test_labels), class_weight=class_weights_dict)

# Evaluate the model
accuracy = ann_model.evaluate(test_features_flat, test_labels, verbose=0)
print(f"Test Accuracy: {accuracy[1] * 100:.2f}%")

# Make predictions on test set
predictions = np.argmax(ann_model.predict(test_features_flat), axis=1)

# Confusion matrix
conf_matrix = confusion_matrix(test_labels_int, predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=train_data.class_indices.keys(), yticklabels=train_data.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Classification report
print("Classification Report:")
print(classification_report(test_labels_int, predictions, zero_division=1))

# Plot Training Loss vs Test Loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title('Train Loss vs Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Training Accuracy vs Test Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Test Accuracy')
plt.title('Train Accuracy vs Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Show the plots
plt.tight_layout()
plt.show()

